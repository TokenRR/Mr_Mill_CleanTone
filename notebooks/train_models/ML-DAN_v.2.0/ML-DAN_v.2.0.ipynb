{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import librosa\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "# train_blended_paths = '../data/audios/english/train/blended'\n",
    "# train_clean_paths = '../data/audios/english/train/clean'\n",
    "train_blended_paths = '../../data/audios/english/train/blended_trim'\n",
    "train_clean_paths = '../../data/audios/english/train/clean_trim'\n",
    "\n",
    "\n",
    "# val_blended = '../data/audios/english/validation/blended'\n",
    "# val_clean = '../data/audios/english/validation/clean'\n",
    "val_blended = '../../data/audios/english/validation/blended_trim'\n",
    "val_clean = '../../data/audios/english/validation/clean_trim'\n",
    "\n",
    "\n",
    "test_blended = '../../data/audios/english/test/blended'\n",
    "test_clean = '../../data/audios/english/test/clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "SAMPLE_RATE = 16000  # Define sample rate for consistency\n",
    "TARGET_LENGTH = SAMPLE_RATE * 3  # Set target length in samples (3 seconds here as an example)\n",
    "batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_tf(path, target_sr=SAMPLE_RATE, target_length=TARGET_LENGTH):\n",
    "    audio, sr = librosa.load(path, sr=target_sr)\n",
    "    # Трімінг або доповнення до потрібної довжини\n",
    "    if len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    else:\n",
    "        audio = np.pad(audio, (0, max(0, target_length - len(audio))))\n",
    "    return audio\n",
    "\n",
    "def get_all_files(directory, extension):\n",
    "    \"\"\"\n",
    "    Рекурсивно отримує всі файли з зазначеним розширенням у директорії та її піддиректоріях.\n",
    "    \"\"\"\n",
    "    file_paths = []\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(extension):\n",
    "                file_paths.append(os.path.join(root, file))\n",
    "    return sorted(file_paths)\n",
    "\n",
    "def audio_to_tf_dataset(blended_dir, clean_dir, batch_size=16, shuffle=True):\n",
    "    blended_files = get_all_files(blended_dir, '.mp3')\n",
    "    clean_files = get_all_files(clean_dir, '.flac')\n",
    "    \n",
    "    assert len(blended_files) == len(clean_files), \"Кількість blended та clean файлів не збігається\"\n",
    "    \n",
    "    def generator():\n",
    "        for b_path, c_path in zip(blended_files, clean_files):\n",
    "            blended_audio = load_audio_tf(b_path)\n",
    "            clean_audio = load_audio_tf(c_path)\n",
    "            yield blended_audio, clean_audio\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(TARGET_LENGTH,), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(TARGET_LENGTH,), dtype=tf.float32),\n",
    "        )\n",
    "    )\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Підготовка датасетів з повторенням\n",
    "train_dataset = audio_to_tf_dataset(train_blended_paths, train_clean_paths, batch_size=16).repeat()\n",
    "val_dataset = audio_to_tf_dataset(val_blended, val_clean, batch_size=batch_size).repeat()\n",
    "\n",
    "# Вказуємо кількість кроків для кожної епохи\n",
    "steps_per_epoch = len(train_dataset) // batch_size\n",
    "validation_steps = len(val_dataset) // batch_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simplified_unet(input_shape=(None, 1)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "\n",
    "    # Зменшимо кількість фільтрів на кожному рівні\n",
    "    conv1 = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "\n",
    "    conv2 = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "\n",
    "    conv3 = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv3)\n",
    "\n",
    "    up4 = layers.UpSampling1D(size=2)(conv3)\n",
    "    up4 = layers.concatenate([up4, conv2])\n",
    "    conv4 = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(up4)\n",
    "    conv4 = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(conv4)\n",
    "\n",
    "    up5 = layers.UpSampling1D(size=2)(conv4)\n",
    "    up5 = layers.concatenate([up5, conv1])\n",
    "    conv5 = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(up5)\n",
    "    conv5 = layers.Conv1D(32, kernel_size=3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    outputs = layers.Conv1D(1, kernel_size=1, activation='linear')(conv5)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ініціалізація та компіляція спрощеної моделі\n",
    "model = simplified_unet(input_shape=(None, 1))\n",
    "model.compile(optimizer='adam', loss='mean_squared_error', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "      7/Unknown \u001b[1m1199s\u001b[0m 168s/step - loss: 0.0042 - mae: 0.0408"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\contextlib.py:155: UserWarning: Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches. You may need to use the `.repeat()` function when building your dataset.\n",
      "  self.gen.throw(typ, value, traceback)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1358s\u001b[0m 194s/step - loss: 0.0041 - mae: 0.0408 - val_loss: 0.0027 - val_mae: 0.0350\n",
      "Epoch 2/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1213s\u001b[0m 172s/step - loss: 0.0023 - mae: 0.0343 - val_loss: 0.0022 - val_mae: 0.0291\n",
      "Epoch 3/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1223s\u001b[0m 175s/step - loss: 0.0019 - mae: 0.0302 - val_loss: 0.0025 - val_mae: 0.0296\n",
      "Epoch 4/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1205s\u001b[0m 172s/step - loss: 0.0016 - mae: 0.0272 - val_loss: 0.0023 - val_mae: 0.0295\n",
      "Epoch 5/5\n",
      "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1206s\u001b[0m 173s/step - loss: 0.0019 - mae: 0.0290 - val_loss: 0.0028 - val_mae: 0.0295\n"
     ]
    }
   ],
   "source": [
    "# Тренування моделі\n",
    "EPOCHS = 5\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Evaluate and save the model\n",
    "model.save(\"ML-DAN_v.2.0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"ML-DAN_v.2.0.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "def load_audio_for_test(file_path, target_sr=SAMPLE_RATE, segment_length=SAMPLE_RATE * 3):\n",
    "    \"\"\"\n",
    "    Завантажує аудіофайл і повертає його сегменти потрібної довжини для подальшої обробки.\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    audio_segments = []\n",
    "\n",
    "    for start in range(0, len(audio), segment_length):\n",
    "        segment = audio[start:start + segment_length]\n",
    "        if len(segment) < segment_length:\n",
    "            segment = np.pad(segment, (0, segment_length - len(segment)))\n",
    "        audio_segments.append(segment.reshape(1, -1, 1))\n",
    "\n",
    "    return audio_segments\n",
    "\n",
    "\n",
    "def denoise_audio(model, input_file):\n",
    "    audio_segments = load_audio_for_test(input_file)\n",
    "    denoised_audio = []\n",
    "\n",
    "    for segment in audio_segments:\n",
    "        denoised_segment = model.predict(segment).squeeze()\n",
    "        denoised_audio.append(denoised_segment)\n",
    "\n",
    "    denoised_audio = np.concatenate(denoised_audio)\n",
    "    output_path = os.path.join(os.path.dirname(input_file), \"denoised_\" + os.path.basename(input_file).split('.')[0] + \".wav\")\n",
    "    sf.write(output_path, denoised_audio, SAMPLE_RATE)\n",
    "    print(f\"Очищене аудіо збережено у файлі: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 2s/step\n",
      "Очищене аудіо збережено у файлі: ../../data/audios/english/train/blended_trim\\denoised_19-198-0002.wav\n"
     ]
    }
   ],
   "source": [
    "denoise_audio(model, '../../data/audios/english/train/blended_trim/19-198-0002.mp3')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
