{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "WMJYncsm6a6i"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import soundfile as sf\n",
    "import math\n",
    "from tensorflow.keras import Model, layers, optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wL219v63-Rnj",
    "outputId": "ea624bd3-cb61-4587-b43a-ceb79ad38c21"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "-pCoWpDV6a6p"
   },
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "\n",
    "train_blended_paths = '../../data/audios/english/train/blended'\n",
    "train_clean_paths = '../../data/audios/english/train/clean'\n",
    "\n",
    "# train_blended_paths = '../../data/audios/english/train/blended_trim'\n",
    "# train_clean_paths = '../../data/audios/english/train/clean_trim'\n",
    "\n",
    "# train_blended_paths = '/content/drive/MyDrive/DAN_data/train/blended_trim'\n",
    "# train_clean_paths = '/content/drive/MyDrive/DAN_data/train/clean_trim'\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "val_blended = '../../data/audios/english/validation/blended'\n",
    "val_clean = '../../data/audios/english/validation/clean'\n",
    "\n",
    "# val_blended = '/content/drive/MyDrive/DAN_data/validation/blended_trim'\n",
    "# val_clean = '/content/drive/MyDrive/DAN_data/validation/clean_trim'\n",
    "\n",
    "# ---------------------------------------------------------------------------\n",
    "\n",
    "test_blended = '../../data/audios/english/test/blended'\n",
    "test_clean = '../../data/audios/english/test/clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "402XyEXG6a6r"
   },
   "outputs": [],
   "source": [
    "# Constants\n",
    "SAMPLE_RATE = 16000  # Define sample rate for consistency\n",
    "TARGET_LENGTH = SAMPLE_RATE * 5  # Set target length in samples (3 seconds here as an example)\n",
    "batch_size = 32\n",
    "reduction_factor = 75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "ttdTke6n6a6t"
   },
   "outputs": [],
   "source": [
    "# Функція для завантаження аудіо та приведення до потрібного формату\n",
    "def load_audio_tf(path, target_sr=SAMPLE_RATE, target_length=TARGET_LENGTH):\n",
    "    audio, sr = librosa.load(path, sr=target_sr)\n",
    "    if len(audio) > target_length:\n",
    "        audio = audio[:target_length]\n",
    "    else:\n",
    "        audio = np.pad(audio, (0, max(0, target_length - len(audio))))\n",
    "    return audio\n",
    "\n",
    "# Функція для отримання всіх файлів у директорії (оригінальна версія без обмеження)\n",
    "def get_all_files(directory, extension=\".mp3\"):\n",
    "    file_paths = {}\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(extension):\n",
    "                file_name = os.path.splitext(file)[0]\n",
    "                file_paths[file_name] = os.path.join(root, file)\n",
    "    return file_paths\n",
    "\n",
    "# Функція для отримання парних файлів з обмеженням розміру набору даних\n",
    "def get_limited_files(directory, extension=\".mp3\", limit=None):\n",
    "    file_paths = {}\n",
    "    count = 0\n",
    "    for root, _, files in os.walk(directory):\n",
    "        for file in files:\n",
    "            if file.endswith(extension):\n",
    "                file_name = os.path.splitext(file)[0]\n",
    "                file_paths[file_name] = os.path.join(root, file)\n",
    "                count += 1\n",
    "                if limit and count >= limit:\n",
    "                    return file_paths\n",
    "    return file_paths\n",
    "\n",
    "# Функція для створення tf.data.Dataset із парних файлів\n",
    "def audio_to_tf_dataset(blended_dir, clean_dir, batch_size=16, shuffle=True, reduction_factor=20):\n",
    "    # Отримуємо обмежену кількість файлів у кожному наборі\n",
    "    blended_files = get_limited_files(blended_dir, '.mp3', limit=None)\n",
    "    clean_files = get_limited_files(clean_dir, '.flac', limit=None)\n",
    "\n",
    "    # Створюємо пари файлів, які мають однакову назву\n",
    "    common_files = list(blended_files.keys() & clean_files.keys())\n",
    "    \n",
    "    # Обмежуємо кількість парних файлів у залежності від reduction_factor\n",
    "    limited_common_files = common_files[::reduction_factor]\n",
    "    \n",
    "    blended_paths = [blended_files[name] for name in limited_common_files]\n",
    "    clean_paths = [clean_files[name] for name in limited_common_files]\n",
    "    \n",
    "    # Створюємо генератор для пар\n",
    "    def generator():\n",
    "        for b_path, c_path in zip(blended_paths, clean_paths):\n",
    "            blended_audio = load_audio_tf(b_path)\n",
    "            clean_audio = load_audio_tf(c_path)\n",
    "            yield blended_audio, clean_audio\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_generator(\n",
    "        generator,\n",
    "        output_signature=(\n",
    "            tf.TensorSpec(shape=(TARGET_LENGTH,), dtype=tf.float32),\n",
    "            tf.TensorSpec(shape=(TARGET_LENGTH,), dtype=tf.float32),\n",
    "        )\n",
    "    )\n",
    "    if shuffle:\n",
    "        dataset = dataset.shuffle(buffer_size=1000)\n",
    "    dataset = dataset.batch(batch_size)\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimized_unet(input_shape=(None, 1)):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    conv1 = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(inputs)\n",
    "    conv1 = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(conv1)\n",
    "    pool1 = layers.MaxPooling1D(pool_size=2)(conv1)\n",
    "\n",
    "    conv2 = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(pool1)\n",
    "    conv2 = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv2)\n",
    "    pool2 = layers.MaxPooling1D(pool_size=2)(conv2)\n",
    "\n",
    "    conv3 = layers.Conv1D(256, kernel_size=3, activation='relu', padding='same')(pool2)\n",
    "    conv3 = layers.Conv1D(256, kernel_size=3, activation='relu', padding='same')(conv3)\n",
    "\n",
    "    up4 = layers.UpSampling1D(size=2)(conv3)\n",
    "    up4 = layers.concatenate([up4, conv2])\n",
    "    conv4 = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(up4)\n",
    "    conv4 = layers.Conv1D(128, kernel_size=3, activation='relu', padding='same')(conv4)\n",
    "\n",
    "    up5 = layers.UpSampling1D(size=2)(conv4)\n",
    "    up5 = layers.concatenate([up5, conv1])\n",
    "    conv5 = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(up5)\n",
    "    conv5 = layers.Conv1D(64, kernel_size=3, activation='relu', padding='same')(conv5)\n",
    "\n",
    "    outputs = layers.Conv1D(1, kernel_size=1, activation='linear')(conv5)\n",
    "\n",
    "    model = Model(inputs, outputs)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "UbmnoHdK6a6u"
   },
   "outputs": [],
   "source": [
    "# Ініціалізація та компіляція моделі\n",
    "model = optimized_unet(input_shape=(TARGET_LENGTH, 1))\n",
    "model.compile(optimizer=optimizers.Adam(learning_rate=1e-4), loss='huber', metrics=['mae'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = audio_to_tf_dataset(train_blended_paths, train_clean_paths, batch_size=batch_size, reduction_factor=reduction_factor).repeat()\n",
    "val_dataset = audio_to_tf_dataset(val_blended, val_clean, batch_size=batch_size, reduction_factor=reduction_factor).repeat()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Підрахунок кількості парних файлів у повному навчальному та валідаційному наборах\n",
    "num_train_samples = len(get_all_files(train_blended_paths, \".mp3\").keys() & get_all_files(train_clean_paths, \".flac\").keys())\n",
    "num_val_samples = len(get_all_files(val_blended, \".mp3\").keys() & get_all_files(val_clean, \".flac\").keys())\n",
    "\n",
    "# Зменшуємо кількість зразків за допомогою reduction_factor\n",
    "num_train_samples = num_train_samples // reduction_factor\n",
    "num_val_samples = num_val_samples // reduction_factor\n",
    "\n",
    "# Обчислення steps_per_epoch та validation_steps\n",
    "steps_per_epoch = max(1, num_train_samples // batch_size)\n",
    "validation_steps = max(1, num_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Підрахунок кількості кроків\n",
    "# num_train_samples = len(train_dataset) * reduction_factor\n",
    "# num_val_samples = len(val_dataset) * reduction_factor\n",
    "\n",
    "steps_per_epoch = max(1, num_train_samples // batch_size)\n",
    "validation_steps = max(1, num_val_samples // batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_rSnvf5E6a6w",
    "outputId": "faf1b3fa-db18-422a-9694-421219b99933"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Кількість пар у тренувальному наборі: 380\n",
      "Кількість пар у валідаційному наборі: 35\n",
      "Кроки на епоху: 11\n",
      "Кроки на валідацію: 1\n"
     ]
    }
   ],
   "source": [
    "print(\"Кількість пар у тренувальному наборі:\", num_train_samples)\n",
    "print(\"Кількість пар у валідаційному наборі:\", num_val_samples)\n",
    "print(\"Кроки на епоху:\", steps_per_epoch)\n",
    "print(\"Кроки на валідацію:\", validation_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3432s\u001b[0m 309s/step - loss: 0.0018 - mae: 0.0330 - val_loss: 0.0027 - val_mae: 0.0367\n",
      "Epoch 2/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3518s\u001b[0m 324s/step - loss: 0.0014 - mae: 0.0296 - val_loss: 0.0014 - val_mae: 0.0242\n",
      "Epoch 3/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3435s\u001b[0m 311s/step - loss: 9.5393e-04 - mae: 0.0267 - val_loss: 0.0014 - val_mae: 0.0277\n",
      "Epoch 4/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3453s\u001b[0m 314s/step - loss: 8.1585e-04 - mae: 0.0250 - val_loss: 5.7297e-04 - val_mae: 0.0196\n",
      "Epoch 5/5\n",
      "\u001b[1m11/11\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3474s\u001b[0m 317s/step - loss: 8.5932e-04 - mae: 0.0255 - val_loss: 0.0011 - val_mae: 0.0254\n"
     ]
    }
   ],
   "source": [
    "# Навчання моделі\n",
    "EPOCHS = 5\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=val_dataset,\n",
    "    epochs=EPOCHS,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    validation_steps=validation_steps\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TBiKvGal6a6y",
    "outputId": "81f108eb-1e21-4480-db34-a9e32dceaa50"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Evaluate and save the model\n",
    "model.save(\"ML-DAN_v.2.0.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "MVPpnMLh6a60"
   },
   "outputs": [],
   "source": [
    "model.save(\"ML-DAN_v.2.0.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "hPFJ1nQI6a61"
   },
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.weights.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "Gtnt16576a61"
   },
   "outputs": [],
   "source": [
    "import soundfile as sf\n",
    "import os\n",
    "\n",
    "def load_audio_for_test(file_path, target_sr=SAMPLE_RATE, segment_length=SAMPLE_RATE * 5):\n",
    "    \"\"\"\n",
    "    Завантажує аудіофайл і повертає його сегменти потрібної довжини для подальшої обробки.\n",
    "    \"\"\"\n",
    "    audio, sr = librosa.load(file_path, sr=target_sr)\n",
    "    audio_segments = []\n",
    "\n",
    "    for start in range(0, len(audio), segment_length):\n",
    "        segment = audio[start:start + segment_length]\n",
    "        if len(segment) < segment_length:\n",
    "            segment = np.pad(segment, (0, segment_length - len(segment)))\n",
    "        audio_segments.append(segment.reshape(1, -1, 1))\n",
    "\n",
    "    return audio_segments\n",
    "\n",
    "\n",
    "def denoise_audio(model, input_file):\n",
    "    audio_segments = load_audio_for_test(input_file)\n",
    "    denoised_audio = []\n",
    "\n",
    "    for segment in audio_segments:\n",
    "        denoised_segment = model.predict(segment).squeeze()\n",
    "        denoised_audio.append(denoised_segment)\n",
    "\n",
    "    denoised_audio = np.concatenate(denoised_audio)\n",
    "    output_path = os.path.join(os.path.dirname(input_file), \"denoised_\" + os.path.basename(input_file).split('.')[0] + \".wav\")\n",
    "    sf.write(output_path, denoised_audio, SAMPLE_RATE)\n",
    "    print(f\"Очищене аудіо збережено у файлі: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7rwQWhDp6a62",
    "outputId": "d0e40333-a574-435b-cab4-4d4f9d356fc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 1s/step\n",
      "Очищене аудіо збережено у файлі: ../../data/audios/english/train/blended\\denoised_19-198-0002.wav\n"
     ]
    }
   ],
   "source": [
    "denoise_audio(model, '../../data/audios/english/train/blended/19-198-0002.mp3')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
