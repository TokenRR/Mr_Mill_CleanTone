{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import librosa\n",
    "import librosa.display\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to data\n",
    "# train_blended_paths = '../data/audios/english/train/blended'\n",
    "# train_clean_paths = '../data/audios/english/train/clean'\n",
    "train_blended_paths = '../data/audios/english/train/blended_trim'\n",
    "train_clean_paths = '../data/audios/english/train/clean_trim'\n",
    "\n",
    "\n",
    "# val_blended = '../data/audios/english/validation/blended'\n",
    "# val_clean = '../data/audios/english/validation/clean'\n",
    "val_blended = '../data/audios/english/validation/blended_trim'\n",
    "val_clean = '../data/audios/english/validation/clean_trim'\n",
    "\n",
    "\n",
    "test_blended = '../data/audios/english/test/blended'\n",
    "test_clean = '../data/audios/english/test/clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to load audio and create spectrogram\n",
    "def load_and_preprocess_audio(filepath, sr=16000, fixed_length=300):\n",
    "    y, _ = librosa.load(filepath, sr=sr)\n",
    "    spectrogram = librosa.stft(y, n_fft=1024, hop_length=512)\n",
    "    spectrogram_db = librosa.amplitude_to_db(np.abs(spectrogram))\n",
    "    \n",
    "    # Adjust the spectrogram length to exactly 300 frames\n",
    "    if spectrogram_db.shape[1] < fixed_length:\n",
    "        # Pad with zeros if it's shorter than the fixed length\n",
    "        padding = fixed_length - spectrogram_db.shape[1]\n",
    "        spectrogram_db = np.pad(spectrogram_db, ((0, 0), (0, padding)), mode='constant')\n",
    "    else:\n",
    "        # Truncate if it's longer than the fixed length\n",
    "        spectrogram_db = spectrogram_db[:, :fixed_length]\n",
    "        \n",
    "    return spectrogram_db\n",
    "\n",
    "\n",
    "# Loading data pairs (blended and clean)\n",
    "def load_data_pairs(blended_path, clean_path):\n",
    "    blended_files = sorted([os.path.join(blended_path, f) for f in os.listdir(blended_path) if f.endswith('.mp3')])\n",
    "    clean_files = sorted([os.path.join(clean_path, f) for f in os.listdir(clean_path) if f.endswith('.flac')])\n",
    "    \n",
    "    blended_spectrograms = [load_and_preprocess_audio(f) for f in blended_files]\n",
    "    clean_spectrograms = [load_and_preprocess_audio(f) for f in clean_files]\n",
    "    \n",
    "    return blended_spectrograms, clean_spectrograms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and validation data\n",
    "train_blended, train_clean = load_data_pairs(train_blended_paths, train_clean_paths)\n",
    "val_blended, val_clean = load_data_pairs(val_blended, val_clean)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "\n",
    "class ResizeLayer(Layer):\n",
    "    def __init__(self, target_height, target_width, **kwargs):\n",
    "        super(ResizeLayer, self).__init__(**kwargs)\n",
    "        self.target_height = target_height\n",
    "        self.target_width = target_width\n",
    "\n",
    "    def call(self, inputs):\n",
    "        return tf.image.resize(inputs, [self.target_height, self.target_width])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = super(ResizeLayer, self).get_config()\n",
    "        config.update({\n",
    "            \"target_height\": self.target_height,\n",
    "            \"target_width\": self.target_width,\n",
    "        })\n",
    "        return config\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Architecture (U-Net for Denoising)\n",
    "def build_unet_model(input_shape):\n",
    "    inputs = layers.Input(shape=input_shape)\n",
    "    \n",
    "    # Downsampling\n",
    "    x1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(inputs)\n",
    "    x1 = layers.MaxPooling2D((2, 2), padding='same')(x1)\n",
    "    \n",
    "    x2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x1)\n",
    "    x2 = layers.MaxPooling2D((2, 2), padding='same')(x2)\n",
    "    \n",
    "    x3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x2)\n",
    "    x3 = layers.MaxPooling2D((2, 2), padding='same')(x3)\n",
    "    \n",
    "    # Bottleneck\n",
    "    b = layers.Conv2D(512, (3, 3), activation='relu', padding='same')(x3)\n",
    "    \n",
    "    # Upsampling\n",
    "    x3 = layers.UpSampling2D((2, 2))(b)\n",
    "    x3 = layers.Conv2D(256, (3, 3), activation='relu', padding='same')(x3)\n",
    "    \n",
    "    x2 = layers.UpSampling2D((2, 2))(x3)\n",
    "    x2 = layers.Conv2D(128, (3, 3), activation='relu', padding='same')(x2)\n",
    "    \n",
    "    x1 = layers.UpSampling2D((2, 2))(x2)\n",
    "    x1 = layers.Conv2D(64, (3, 3), activation='relu', padding='same')(x1)\n",
    "    \n",
    "    # Final layer with the same shape as input, followed by resizing layer\n",
    "    outputs = layers.Conv2D(1, (1, 1), activation='linear', padding='same')(x1)\n",
    "    outputs = ResizeLayer(target_height=input_shape[0], target_width=input_shape[1])(outputs)  # Resize to exact input shape\n",
    "    \n",
    "    model = models.Model(inputs, outputs)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From d:\\KPI\\Machine_learning\\ML-DAN\\.venv\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:204: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Define fixed input shape for spectrograms\n",
    "input_shape = (513, 300, 1)  # (frequency_bins, time_frames, 1)\n",
    "\n",
    "# Update model function to use the fixed input shape\n",
    "model = build_unet_model(input_shape)\n",
    "model.compile(optimizer=Adam(learning_rate=0.001), loss='mean_squared_error')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data in batches (generator)\n",
    "def data_generator(blended, clean, batch_size=8, fixed_length=300):\n",
    "    while True:\n",
    "        for i in range(0, len(blended), batch_size):\n",
    "            x_batch = np.array([np.expand_dims(b, -1) for b in blended[i:i + batch_size]])\n",
    "            y_batch = np.array([np.expand_dims(c, -1) for c in clean[i:i + batch_size]])\n",
    "            yield x_batch, y_batch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "batch_size = 8\n",
    "train_gen = data_generator(train_blended, train_clean, batch_size)\n",
    "val_gen = data_generator(val_blended, val_clean, batch_size)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m62s\u001b[0m 62s/step - loss: 856.0461 - val_loss: 2982.1938\n",
      "Epoch 2/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 1809.9778 - val_loss: 390.7227\n",
      "Epoch 3/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 42s/step - loss: 623.2143 - val_loss: 555.6476\n",
      "Epoch 4/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 612.6620 - val_loss: 511.6505\n",
      "Epoch 5/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 37s/step - loss: 838.0853 - val_loss: 579.9418\n",
      "Epoch 6/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 631.6924 - val_loss: 512.3835\n",
      "Epoch 7/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 37s/step - loss: 839.4728 - val_loss: 569.4586\n",
      "Epoch 8/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 621.7940 - val_loss: 492.6734\n",
      "Epoch 9/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 804.3489 - val_loss: 523.6749\n",
      "Epoch 10/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 9s/step - loss: 578.1863 - val_loss: 420.1073\n",
      "Epoch 11/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 676.3151 - val_loss: 361.7633\n",
      "Epoch 12/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 424.6267 - val_loss: 196.9272\n",
      "Epoch 13/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 39s/step - loss: 289.9210 - val_loss: 195.9666\n",
      "Epoch 14/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 189.9905 - val_loss: 494.3111\n",
      "Epoch 15/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 932.7154 - val_loss: 198.5117\n",
      "Epoch 16/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 183.1154 - val_loss: 139.0561\n",
      "Epoch 17/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 195.8215 - val_loss: 201.5816\n",
      "Epoch 18/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 279.1509 - val_loss: 244.9938\n",
      "Epoch 19/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 39s/step - loss: 370.0814 - val_loss: 286.7374\n",
      "Epoch 20/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - loss: 369.1025 - val_loss: 275.7344\n",
      "Epoch 21/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 422.4396 - val_loss: 273.8828\n",
      "Epoch 22/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 362.4910 - val_loss: 226.9401\n",
      "Epoch 23/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 337.3549 - val_loss: 179.2151\n",
      "Epoch 24/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 271.1853 - val_loss: 129.4937\n",
      "Epoch 25/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 37s/step - loss: 177.6658 - val_loss: 173.0021\n",
      "Epoch 26/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 180.8083 - val_loss: 254.1541\n",
      "Epoch 27/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 39s/step - loss: 454.4356 - val_loss: 217.7484\n",
      "Epoch 28/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 185.5594 - val_loss: 127.8298\n",
      "Epoch 29/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 190.2708 - val_loss: 133.5774\n",
      "Epoch 30/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 192.6033 - val_loss: 145.2841\n",
      "Epoch 31/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 203.3828 - val_loss: 162.7804\n",
      "Epoch 32/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 233.2416 - val_loss: 162.8748\n",
      "Epoch 33/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 232.6793 - val_loss: 157.6745\n",
      "Epoch 34/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 221.2445 - val_loss: 138.3430\n",
      "Epoch 35/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 37s/step - loss: 195.3179 - val_loss: 131.5499\n",
      "Epoch 36/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 171.7420 - val_loss: 128.0919\n",
      "Epoch 37/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 195.3476 - val_loss: 163.0230\n",
      "Epoch 38/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 12s/step - loss: 159.0520 - val_loss: 145.2171\n",
      "Epoch 39/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 233.8602 - val_loss: 147.2030\n",
      "Epoch 40/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 154.3007 - val_loss: 120.1907\n",
      "Epoch 41/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 39s/step - loss: 175.6535 - val_loss: 125.9352\n",
      "Epoch 42/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 166.3360 - val_loss: 121.9151\n",
      "Epoch 43/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 170.4804 - val_loss: 125.7983\n",
      "Epoch 44/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 174.4925 - val_loss: 117.9629\n",
      "Epoch 45/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 165.1900 - val_loss: 122.7285\n",
      "Epoch 46/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 159.1421 - val_loss: 117.9765\n",
      "Epoch 47/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 175.0294 - val_loss: 128.7482\n",
      "Epoch 48/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 11s/step - loss: 149.6985 - val_loss: 115.3752\n",
      "Epoch 49/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 38s/step - loss: 170.4334 - val_loss: 121.7728\n",
      "Epoch 50/50\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 10s/step - loss: 150.0196 - val_loss: 111.8283\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    train_gen, \n",
    "    epochs=50, \n",
    "    steps_per_epoch=len(train_blended) // batch_size,\n",
    "    validation_data=val_gen, \n",
    "    validation_steps=len(val_blended) // batch_size\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "# Evaluate and save the model\n",
    "model.save(\"audio_denoising_unet.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example inference function\n",
    "def denoise_audio(model, blended_audio):\n",
    "    spectrogram = load_and_preprocess_audio(blended_audio)\n",
    "    input_spec = np.expand_dims(spectrogram, axis=[0, -1])\n",
    "    denoised_spec = model.predict(input_spec)\n",
    "    denoised_audio = librosa.istft(denoised_spec[0, ..., 0])\n",
    "    return denoised_audio\n",
    "\n",
    "# Usage example (replace with an actual audio file path):\n",
    "# denoised_audio = denoise_audio(model, '../data/audios/english/test/blended/sample.mp3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"audio_denoising_unet.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_weights('model_weights.weights.h5')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
