{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import librosa\n",
    "import librosa.display\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers, models\n",
    "import soundfile as sf\n",
    "from typing import Tuple, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_file(file_path: str, sr: int = 16000) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Load an audio file with a specified sample rate.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the audio file.\n",
    "        sr (int): Sample rate for loading the audio.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Loaded audio signal.\n",
    "    \"\"\"\n",
    "    audio, _ = librosa.load(file_path, sr=sr)\n",
    "    return audio\n",
    "\n",
    "def audio_to_spectrogram(audio: np.ndarray, n_fft: int = 1024, hop_length: int = 512) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert audio signal to a spectrogram.\n",
    "    \n",
    "    Args:\n",
    "        audio (np.ndarray): Audio signal.\n",
    "        n_fft (int): Number of FFT components.\n",
    "        hop_length (int): Number of samples between successive frames.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Spectrogram of the audio.\n",
    "    \"\"\"\n",
    "    stft = librosa.stft(audio, n_fft=n_fft, hop_length=hop_length)\n",
    "    spectrogram = np.abs(stft)\n",
    "    return spectrogram\n",
    "\n",
    "def spectrogram_to_audio(spectrogram: np.ndarray, hop_length: int = 512) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Convert a spectrogram back to an audio signal.\n",
    "    \n",
    "    Args:\n",
    "        spectrogram (np.ndarray): Spectrogram.\n",
    "        hop_length (int): Number of samples between successive frames.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Reconstructed audio signal.\n",
    "    \"\"\"\n",
    "    stft_reconstructed = librosa.istft(spectrogram, hop_length=hop_length)\n",
    "    return stft_reconstructed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(\n",
    "    noise_path: str,\n",
    "    clean_path: str,\n",
    "    noisy_path: str,\n",
    "    limit: Optional[int] = None,\n",
    "    fixed_length: int = 300\n",
    ") -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Load noise, clean, and noisy audio data and convert them to spectrograms with a fixed length.\n",
    "    \n",
    "    Args:\n",
    "        noise_path (str): Path to noise recordings.\n",
    "        clean_path (str): Path to clean recordings.\n",
    "        noisy_path (str): Path to noisy recordings.\n",
    "        limit (Optional[int]): Limit on the number of files to load.\n",
    "        fixed_length (int): Fixed time dimension length for spectrograms.\n",
    "    \n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray, np.ndarray]: Arrays of noise, clean, and noisy spectrograms.\n",
    "    \"\"\"\n",
    "    noise_files = sorted([os.path.join(noise_path, f) for f in os.listdir(noise_path) if f.endswith('.wav')])[:limit]\n",
    "    clean_files = sorted([os.path.join(clean_path, f) for f in os.listdir(clean_path) if f.endswith('.flac')])[:limit]\n",
    "    noisy_files = sorted([os.path.join(noisy_path, f) for f in os.listdir(noisy_path) if f.endswith('.mp3')])[:limit]\n",
    "    \n",
    "    def pad_or_truncate(spectrogram: np.ndarray, target_length: int) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Pad or truncate a spectrogram to ensure a consistent length along the time axis.\n",
    "        \n",
    "        Args:\n",
    "            spectrogram (np.ndarray): Input spectrogram.\n",
    "            target_length (int): Desired length along the time axis.\n",
    "        \n",
    "        Returns:\n",
    "            np.ndarray: Padded or truncated spectrogram.\n",
    "        \"\"\"\n",
    "        if spectrogram.shape[1] > target_length:\n",
    "            return spectrogram[:, :target_length]\n",
    "        else:\n",
    "            padding = target_length - spectrogram.shape[1]\n",
    "            return np.pad(spectrogram, ((0, 0), (0, padding)), mode='constant')\n",
    "    \n",
    "    # Convert audio to spectrograms and fix length\n",
    "    noise_spectrograms = [pad_or_truncate(audio_to_spectrogram(load_audio_file(f)), fixed_length) for f in noise_files]\n",
    "    clean_spectrograms = [pad_or_truncate(audio_to_spectrogram(load_audio_file(f)), fixed_length) for f in clean_files]\n",
    "    noisy_spectrograms = [pad_or_truncate(audio_to_spectrogram(load_audio_file(f)), fixed_length) for f in noisy_files]\n",
    "    \n",
    "    # Convert lists to numpy arrays with consistent shapes\n",
    "    return np.array(noise_spectrograms), np.array(clean_spectrograms), np.array(noisy_spectrograms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_denoising_cnn(input_shape: Tuple[int, int, int]) -> tf.keras.Model:\n",
    "    \"\"\"\n",
    "    Define a CNN model for audio noise reduction with precise shape matching adjustments.\n",
    "    \n",
    "    Args:\n",
    "        input_shape (Tuple[int, int, int]): Shape of the input spectrogram (height, width, channels).\n",
    "    \n",
    "    Returns:\n",
    "        tf.keras.Model: Compiled denoising CNN model.\n",
    "    \"\"\"\n",
    "    model = models.Sequential([\n",
    "        layers.Input(shape=input_shape),\n",
    "        \n",
    "        # Encoder\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2), padding='same'),  # Reduce size by half\n",
    "        \n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2), padding='same'),  # Reduce size by half again\n",
    "        \n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2D(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.MaxPooling2D((2, 2), padding='same'),  # Reduce size by half once more\n",
    "        \n",
    "        # Decoder\n",
    "        layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2DTranspose(128, (3, 3), activation='relu', padding='same'),\n",
    "        layers.UpSampling2D((2, 2)),  # Restore original size\n",
    "        \n",
    "        layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2DTranspose(64, (3, 3), activation='relu', padding='same'),\n",
    "        layers.UpSampling2D((2, 2)),  # Restore size further\n",
    "        \n",
    "        layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.Conv2DTranspose(32, (3, 3), activation='relu', padding='same'),\n",
    "        layers.UpSampling2D((2, 2)),  # Final upsampling to match input size\n",
    "        \n",
    "        # Output layer\n",
    "        layers.Conv2D(1, (3, 3), activation='sigmoid', padding='same')\n",
    "    ])\n",
    "    \n",
    "    # Confirm model output shape\n",
    "    print(\"Expected input shape:\", input_shape)\n",
    "    print(\"Model output shape:\", model.output_shape)\n",
    "    \n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data shapes:\n",
      "train_noisy: (100, 513, 300, 1)\n",
      "train_clean: (100, 513, 300, 1)\n",
      "Validation data shapes:\n",
      "val_noisy: (100, 513, 300, 1)\n",
      "val_clean: (100, 513, 300, 1)\n",
      "Expected input shape: (513, 300, 1)\n",
      "Model output shape: (None, 520, 304, 1)\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Dimensions must be equal, but are 513 and 520 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](data_1, sequential_4_1/conv2d_25_1/Sigmoid)' with input shapes: [?,513,300,1], [?,520,304,1].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 31\u001b[0m\n\u001b[0;32m     29\u001b[0m input_shape \u001b[38;5;241m=\u001b[39m train_noisy\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m:]  \u001b[38;5;66;03m# Use shape of one sample\u001b[39;00m\n\u001b[0;32m     30\u001b[0m model \u001b[38;5;241m=\u001b[39m build_denoising_cnn(input_shape)\n\u001b[1;32m---> 31\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_clean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mval_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_clean\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\KPI\\Machine_learning\\ML-DAN\\.venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32md:\\KPI\\Machine_learning\\ML-DAN\\.venv\\Lib\\site-packages\\keras\\src\\losses\\losses.py:1303\u001b[0m, in \u001b[0;36mmean_squared_error\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m   1301\u001b[0m y_true \u001b[38;5;241m=\u001b[39m ops\u001b[38;5;241m.\u001b[39mconvert_to_tensor(y_true, dtype\u001b[38;5;241m=\u001b[39my_pred\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m   1302\u001b[0m y_true, y_pred \u001b[38;5;241m=\u001b[39m squeeze_or_expand_to_same_rank(y_true, y_pred)\n\u001b[1;32m-> 1303\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ops\u001b[38;5;241m.\u001b[39mmean(ops\u001b[38;5;241m.\u001b[39msquare(\u001b[43my_true\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m), axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Dimensions must be equal, but are 513 and 520 for '{{node compile_loss/mse/sub}} = Sub[T=DT_FLOAT](data_1, sequential_4_1/conv2d_25_1/Sigmoid)' with input shapes: [?,513,300,1], [?,520,304,1]."
     ]
    }
   ],
   "source": [
    "# Set paths for training and validation data\n",
    "train_noise_path = '../../../data/audios/english/train/noise'\n",
    "train_clean_path = '../../../data/audios/english/train/clean_trim'\n",
    "train_noisy_path = '../../../data/audios/english/train/blended_trim'\n",
    "\n",
    "val_noise_path = '../../../data/audios/english/validation/noise'\n",
    "val_clean_path = '../../../data/audios/english/validation/clean_trim'\n",
    "val_noisy_path = '../../../data/audios/english/validation/blended_trim'\n",
    "\n",
    "# Load training and validation data (limit as needed)\n",
    "train_noise, train_clean, train_noisy = load_data(train_noise_path, train_clean_path, train_noisy_path, limit=500, fixed_length=300)\n",
    "val_noise, val_clean, val_noisy = load_data(val_noise_path, val_clean_path, val_noisy_path, limit=100, fixed_length=300)\n",
    "\n",
    "# Add channel dimension for CNN input\n",
    "train_noisy = train_noisy[..., np.newaxis]\n",
    "train_clean = train_clean[..., np.newaxis]\n",
    "val_noisy = val_noisy[..., np.newaxis]\n",
    "val_clean = val_clean[..., np.newaxis]\n",
    "\n",
    "# Debugging statements to inspect shapes\n",
    "print(\"Training data shapes:\")\n",
    "print(\"train_noisy:\", train_noisy.shape)\n",
    "print(\"train_clean:\", train_clean.shape)\n",
    "print(\"Validation data shapes:\")\n",
    "print(\"val_noisy:\", val_noisy.shape)\n",
    "print(\"val_clean:\", val_clean.shape)\n",
    "\n",
    "# Build and train model\n",
    "input_shape = train_noisy.shape[1:]  # Use shape of one sample\n",
    "model = build_denoising_cnn(input_shape)\n",
    "history = model.fit(train_noisy, train_clean, validation_data=(val_noisy, val_clean), epochs=20, batch_size=8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model to both .h5 and .keras formats\n",
    "model.save('ML-DAN_v3.0.h5')\n",
    "model.save('ML-DAN_v3.0.keras')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def denoise_audio(model: tf.keras.Model, noisy_audio_path: str, output_path: str) -> None:\n",
    "    \"\"\"\n",
    "    Apply noise reduction on a noisy audio file and save the cleaned audio.\n",
    "    \n",
    "    Args:\n",
    "        model (tf.keras.Model): Trained noise reduction model.\n",
    "        noisy_audio_path (str): Path to the noisy audio file.\n",
    "        output_path (str): Path where the denoised audio will be saved.\n",
    "    \"\"\"\n",
    "    # Load and convert noisy audio to spectrogram\n",
    "    noisy_audio = load_audio_file(noisy_audio_path)\n",
    "    noisy_spectrogram = audio_to_spectrogram(noisy_audio)\n",
    "    \n",
    "    # Add batch and channel dimension for model prediction\n",
    "    noisy_spectrogram = noisy_spectrogram[np.newaxis, ..., np.newaxis]\n",
    "    \n",
    "    # Predict clean spectrogram\n",
    "    predicted_clean_spectrogram = model.predict(noisy_spectrogram)[0, ..., 0]\n",
    "    \n",
    "    # Convert predicted spectrogram back to audio\n",
    "    cleaned_audio = spectrogram_to_audio(predicted_clean_spectrogram)\n",
    "    \n",
    "    # Save the cleaned audio\n",
    "    sf.write(output_path, cleaned_audio, 16000)\n",
    "\n",
    "# Example usage of the denoising function\n",
    "denoise_audio(model, 'path_to_noisy_audio.mp3', 'path_to_cleaned_output.wav')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
